{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import json\n",
    "from transformers import LlamaTokenizer\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = json.load(open('tmp5.json'))\n",
    "log = json.load(open('log.json'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def DataProcess(data,index,log,labels_path=None):\n",
    "    \n",
    "    print('\\n\\nImporting and filtering database...')\n",
    "        \n",
    "    ints_str = '0123456789-#[]' # characters that usually main categories don't start with\n",
    "    notes = data\n",
    "    section = log[index][0]['section_used']\n",
    "    print('\\n\\nSplitting each note into sections:\\n\\n')\n",
    "    \n",
    "    notes_sections = {}\n",
    "    \n",
    "    for note_index in tqdm(range(notes.shape[0])):\n",
    "        \n",
    "        note = notes['text'][note_index].replace('\\n\\n\\n\\n','\\n').replace('\\n\\n\\n','\\n').replace('     ','\\n')\n",
    "        paragraphs = note.split('\\n')\n",
    "        \n",
    "        subsections, new_section = [], ' '\n",
    "        for p in paragraphs:\n",
    "            line = p.strip()\n",
    "            if len(line)>0 and ':' in line and not (line[line.find(':')-1] in ints_str) and not(line[0] in ints_str):\n",
    "                subsections.append([new_section.strip()])\n",
    "                new_section = p + ' '\n",
    "            else:\n",
    "                new_section += p + ' '\n",
    "        subsections.append([new_section])\n",
    "        subsections.pop(0)\n",
    "\n",
    "        note_sect_tit,note_sect_par = [],[]\n",
    "        for sect in subsections:\n",
    "            note_sect_tit += [str(*sect)[0:str(*sect).find(':')]]\n",
    "            note_sect_par += [str(*sect)[str(*sect).find(':')+1:].strip()]\n",
    "        note_df = pd.DataFrame({'title':note_sect_tit,'category':'','text':note_sect_par, 'label':''})\n",
    "        notes_sections[notes['note_id'][note_index]] = note_df\n",
    "    \n",
    "    f = open(labels_path, 'r')\n",
    "    obj_label = f.readlines()\n",
    "    obj_label_dict = {}\n",
    "    i = 0\n",
    "    for s in obj_label:\n",
    "        i += 1\n",
    "        if '/' in s:\n",
    "            buffer = s.strip('\\n').lower().split('/')\n",
    "            for item in buffer:\n",
    "                obj_label_dict[item] = i\n",
    "        else:\n",
    "            obj_label_dict[s.strip('\\n').lower()] = i\n",
    "    f.close()\n",
    "\n",
    "    for key in tqdm(list(notes_sections.keys())):\n",
    "        buffer = 'begin_title'\n",
    "        t = list(notes_sections[key]['title'])\n",
    "        for idx in range(len(t)):\n",
    "            for item in list(obj_label_dict.keys()):\n",
    "                if item in t[idx].lower() and len(t[idx].lower())>2:\n",
    "                    buffer = item\n",
    "                    notes_sections[key]['category'][idx] = buffer\n",
    "                    notes_sections[key]['label'][idx] = obj_label_dict[buffer]\n",
    "                    break\n",
    "            notes_sections[key]['category'][idx] = buffer\n",
    "            notes_sections[key]['label'][idx] = obj_label_dict[buffer]\n",
    "\n",
    "    notes_sections_output = {}\n",
    "    row_id  = notes_sections.keys()\n",
    "    for key in tqdm(row_id):\n",
    "        buffer = ''\n",
    "        note_sect_tit, note_sect_par, note_sect_lab = [], [], []\n",
    "        for i in range(len(notes_sections[key]['category'])):\n",
    "            if buffer != notes_sections[key]['category'][i]:\n",
    "                buffer = notes_sections[key]['category'][i]\n",
    "                note_sect_tit.append(buffer)\n",
    "                note_sect_lab.append(notes_sections[key]['title'][i])\n",
    "                note_sect_par.append(notes_sections[key]['text'][i])\n",
    "            else:\n",
    "                note_sect_par[-1] = note_sect_par[-1] + ' ' + notes_sections[key]['title'][i] + ' ' + notes_sections[key]['text'][i]\n",
    "        note_df = pd.DataFrame({'title': note_sect_tit, 'text': note_sect_par, 'label': note_sect_lab})\n",
    "        notes_sections_output[key] = note_df\n",
    "\n",
    "\n",
    "    notes_sections = notes_sections_output[index]\n",
    "    \n",
    "    notes_sections = notes_sections[notes_sections['label'].isin(section)]\n",
    "    \n",
    "    return ''.join(notes_sections['label'] + \": \" + notes_sections['text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Process(data,log):\n",
    "    \n",
    "    for i,key in enumerate(data['data']):\n",
    "        index = data['data'][i]['title']\n",
    "        temp = data['data'][i]['paragraphs'][0]['context']\n",
    "        temp = pd.DataFrame({'note_id':[index],'text':[temp]})\n",
    "        context = DataProcess(temp,index,log,\"labels.txt\")\n",
    "        data['data'][i]['paragraphs'][0]['context'] = context\n",
    "        \n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:00<00:00, 136.39it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00, 40.05it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00, 947.01it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00, 1000.07it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00, 49.04it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00, 833.20it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00, 1254.65it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00, 77.97it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00, 220.36it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00, 1154.82it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00, 69.71it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00, 825.65it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00, 1575.62it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00, 51.07it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00, 218.43it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00, 1054.91it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00, 75.64it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00, 1070.25it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00, 1540.89it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "Importing and filtering database...\n",
      "\n",
      "\n",
      "Splitting each note into sections:\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Importing and filtering database...\n",
      "\n",
      "\n",
      "Splitting each note into sections:\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Importing and filtering database...\n",
      "\n",
      "\n",
      "Splitting each note into sections:\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Importing and filtering database...\n",
      "\n",
      "\n",
      "Splitting each note into sections:\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Importing and filtering database...\n",
      "\n",
      "\n",
      "Splitting each note into sections:\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Importing and filtering database...\n",
      "\n",
      "\n",
      "Splitting each note into sections:\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Importing and filtering database...\n",
      "\n",
      "\n",
      "Splitting each note into sections:\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "100%|██████████| 1/1 [00:00<00:00, 63.16it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00, 826.63it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00, 1353.87it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00, 76.38it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00, 764.97it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00, 1329.41it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00, 73.22it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00, 896.22it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00, 1381.07it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00, 63.37it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00, 796.19it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "Importing and filtering database...\n",
      "\n",
      "\n",
      "Splitting each note into sections:\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Importing and filtering database...\n",
      "\n",
      "\n",
      "Splitting each note into sections:\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Importing and filtering database...\n",
      "\n",
      "\n",
      "Splitting each note into sections:\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "data = Process(data,log)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The tokenizer class you load from this checkpoint is not the same type as the class this function is called from. It may result in unexpected tokenization. \n",
      "The tokenizer class you load from this checkpoint is 'LLaMATokenizer'. \n",
      "The class this function is called from is 'LlamaTokenizer'.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1314\n",
      "1054\n",
      "1384\n",
      "1381\n",
      "1069\n",
      "679\n",
      "1327\n",
      "1394\n",
      "1266\n",
      "1108\n"
     ]
    }
   ],
   "source": [
    "tokenizer = LlamaTokenizer.from_pretrained(\"decapoda-research/llama-7b-hf\")\n",
    "for i,key in enumerate(data['data']):\n",
    "    print(len(tokenizer.tokenize(data['data'][i]['paragraphs'][0]['context'])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('QA.json', 'w') as f:      \n",
    "    json.dump(data, f, indent=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1314\n",
      "1054\n",
      "1384\n",
      "1381\n",
      "1069\n",
      "679\n",
      "1327\n",
      "1394\n",
      "1266\n",
      "1108\n"
     ]
    }
   ],
   "source": [
    "mimic4_qa = []\n",
    "for i,key in enumerate(data['data']):\n",
    "    instruction = data['data'][i]['paragraphs'][0]['context']\n",
    "    print(len(tokenizer.tokenize(instruction)))\n",
    "    \n",
    "    for(j,question) in enumerate(data['data'][i]['paragraphs'][0]['qas']):\n",
    "        #print(j)\n",
    "        question = question['question']\n",
    "        answer = data['data'][i]['paragraphs'][0]['qas'][j]['answers']\n",
    "        mimic4_qa.append({'instruction':instruction,'input':question,'output':answer})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('mimic4_qa.json', 'w') as f:      \n",
    "    json.dump(mimic4_qa, f, indent=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1314"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(tokenizer.tokenize(\"Allergies: Sulfa (Sulfonamides) / Erythromycin BaseAttending: ___.History of Present Illness: Patient is a ___ year old female status post uterine artery  fibroid embolization by ___ on ___ who was discharged ___ and  around midnight awoke with right groin pain. The patient noticed  that her right groin appeared swollen and felt slightly firm.  This area of swelling seemed to increase in size. The patient  also felt pain radiating across her lower abdomen, causing her  to \\\"double over in pain.\\\" She also felt it difficult to  straighten her right leg without pain. She called the  interventional radiology physician's assistant, who instructed  her to come to the emergency department.  In the ED, patient was found by ultrasound to have a right groin  pseudoaneurysm. Patient was admitted for thrombin injection of  pseudoaneurysm.  On the floor, patient reported right groin pain of ___. She had  no other complaints. Review of sytems (+) Per HPI (-) Denies fever, chills, night sweats, recent weight loss or  gain. Denies headache, sinus tenderness, rhinorrhea or  congestion. Denied cough, shortness of breath. Denied chest pain  or tightness, palpitations. Denied nausea, vomiting, diarrhea,  constipation or abdominal pain. No recent change in bowel or  bladder habits. No dysuria.Past Medical History: 1. Rheumatoid Arthrits 2. Irritable Bowel Syndrome 3. Bicuspid aortic valve (no TTE in chart).  Past Surgical History 1. Bilateral knee arthroscopy 2. Bilateral ___ Metatarsal surgerySocial History: ___Physical Exam:  Vitals T: 98.4 BP:96/60 P: 63 R:18 O2: 98 RA General Alert, oriented, no acute distress HEENT Sclera anicteric, MMM, oropharynx clear Neck supple, JVP not elevated, no LAD Lungs Clear to auscultation bilaterally, no wheezes, rales,  ronchi CV Regular rate and rhythm, normal S1 + S2, no murmurs, rubs,  gallops Abdomen soft, non-tender, non-distended, bowel sounds present,  no rebound tenderness or guarding, no organomegaly Ext Warm, well perfused, 2+ pulses, no clubbing, cyanosis or  edema Right Groin Appears slightly swollen, no erythema, +bruit  auscultated, +tenderness to palpation.Brief Hospital Course: This is a ___ year old female with uterine fibroid embolization  on ___ who presented with right groin pain and swelling  determined by ultrasound to be a pseudoaneurysm.  1) Pseudoaneurysm: Patient presented with psuedoaneurysm  confirmed by ultrasound that was secondary to her recent uterine  fibroid embolization on ___. On ___ patient underwent  psuedoaneurysm thrombin injection. Ultrasound on ___ indicated  pseudoaneurysm had been thrombosed with remaining femoral artery  patent. A repeat US on ___ prior to discharge also demonstrated  thrombosed right femoral artery  pseudoaneurysm and patent right common and superficial femoral  arteries and veins. Following thrombin injection patient  experienced right groin pain. She also experienced pain in her  pelvic region thought to be secondary to fibroids responding to  embolization procedure. Pain initially treated with intravenous  morphine and transitioned to oral oxycodone at time of  discharge. She was given a prescription for oxycodone 5mg ___  tabs TID as needed for pain. She was instructed not to drive or  operate heavy machinery while taking this medication. She was  also prescribed ibuprofen 800 mg PO TID to reduce inflammation  and instructed to take this medication with food and water.  Post-procedure follow up to be arranged by ___,  interventional radiology ___.  2) Constipation: Patient experienced constipation during this  admission. Last bowel movement day prior to admission. Patient  reports that she has irritable bowel syndrome with constipation  at baseline. Likely constipation worsened due to patient taking  narcotics for pain control. Patient started on bowel regimen  including colace, senna and lactulose. Patient was discharged  with a prescription for oral lactulose. She will contact her  primary care physician if she is unable to have a bowel movement  within ___ hours of discharge.    3)Rheumatoid Arthritis:Stable during this admission. Patient was  continued on outpatient regimen of prednisone and folate.  Patient had taken weekly dose of methotrexate prior to  admission.Medications on Admission: 1. Prednisone 5mg QDAY 2. Folate 1 mg QDAY 3. Methotrexate 10mg Q wk 4. Culturale (probiotic) dailyDischarge Disposition: Home\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
